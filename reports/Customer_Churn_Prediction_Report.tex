% Customer_Churn_Prediction_Report.tex
% Compact IEEE layout with small vertical gap after each \subsection

\documentclass[conference]{IEEEtran}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{url}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{microtype}
\usepackage{siunitx}
\usepackage{float}

\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    linkcolor=black,
    citecolor=black
}

% -------------------------------------------------------------------
% Compact layout tuning
\setlength{\parskip}{0pt}
\setlength{\parindent}{1em}
\setlength{\textfloatsep}{4pt plus 1pt minus 1pt}
\setlength{\floatsep}{4pt plus 1pt minus 1pt}
\setlength{\intextsep}{4pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{3pt}
\setlength{\belowcaptionskip}{3pt}
\setlist[itemize]{topsep=0pt,partopsep=0pt,itemsep=1pt,parsep=0pt,leftmargin=*}
\setlist[enumerate]{topsep=0pt,partopsep=0pt,itemsep=1pt,parsep=0pt,leftmargin=*}

% Tighten section spacing and add small gap after subsections
\makeatletter
\def\section{\@startsection {section}{1}{\z@}
  {0.2ex plus .2ex minus .2ex}
  {0.2ex plus .2ex}
  {\normalfont\normalsize\bfseries}}
\def\subsection{\@startsection{subsection}{2}{\z@}
  {0.2ex plus .2ex minus .2ex}
  {6pt plus 1pt minus 1pt}
  {\normalfont\normalsize\itshape}}
\makeatother

% -------------------------------------------------------------------
\title{Customer Churn Prediction Using Machine Learning}

\author{%
\IEEEauthorblockN{Monika Tyagi}
\IEEEauthorblockA{Indian Institute of Science\\
Email: monikatyagi@iisc.ac.in}
\and
\IEEEauthorblockN{Sourajit Bhar}
\IEEEauthorblockA{Indian Institute of Science\\
Email: sourajitbhar@iisc.ac.in}
}

\begin{document}
\sloppy
\maketitle

\begin{abstract}
Customer churn prediction is a critical task for subscription-based businesses. In this project, we analyze the Telco Customer Churn dataset and apply multiple supervised machine learning models --- including Logistic Regression, Decision Tree, Random Forest, Naive Bayes, and Support Vector Machine (SVM) --- to detect at-risk customers. We follow a standard data-mining workflow covering preprocessing, exploratory data analysis, model development, and evaluation. Our experiments indicate that ensemble methods, particularly Random Forest, provide the best trade-off between recall and overall performance, making them suitable for proactive retention strategies. Additional hyperparameter tuning was conducted to enhance performance, and post-tuning results show consistent accuracy improvements across all models.
\end{abstract}

\begin{IEEEkeywords}
Customer Churn, Classification, Data Mining, Machine Learning, Random Forest, Hyperparameter Tuning, Telco
\end{IEEEkeywords}

% -------------------------------------------------------------------
\section{Introduction}
Customer churn refers to the phenomenon in which customers discontinue a service. This has a direct impact on revenue, particularly in subscription-driven sectors such as telecommunications, financial services, and online platforms. Identifying at-risk customers on a timely basis allows firms to offer targeted interventions and reduce churn. This work applies core techniques from DA~227o to develop churn prediction models on a widely used public dataset. Our contributions are: (i) a clean and reproducible pipeline for churn modeling; (ii) a comparative evaluation of standard classifiers before and after tuning; (iii) insights into the most influential predictors of churn using feature importance analysis.

% -------------------------------------------------------------------
\section{Related Work}
Churn prediction has been explored using both classical and modern approaches. Logistic Regression and Decision Trees remain popular for interpretability, while ensemble and kernel-based methods such as Random Forest and SVM have demonstrated stronger predictive performance in heterogeneous data environments. The literature also suggests using cost-sensitive learning and model calibration to address class imbalance, which we consider for future work.

% -------------------------------------------------------------------
\section{Dataset}
We use the \emph{Telco Customer Churn} dataset (Kaggle), which contains 7{,}043 customer records with demographic details such as gender, age range, and whether they have partners and dependents; service usage features (phone, internet, online security, online backup, device protection, tech support, streaming services); account information (tenure, contract, payment method, paperless billing, monthly and total charges); and churn labels. The target variable is binary: \texttt{Churn = Yes/No}. Data preprocessing included:
\begin{itemize}
    \item Removal of redundant columns (e.g., \texttt{customerID})
    \item Handling of missing values in \texttt{TotalCharges}
    \item One-hot encoding of categorical features
    \item Normalization of continuous attributes such as \texttt{tenure} and \texttt{MonthlyCharges}
\end{itemize}

\section{Methodology}
Our modeling pipeline follows a structured workflow:
\begin{enumerate}
    \item \textbf{Data Cleaning and Preprocessing:} Data consistency checks, imputation, and feature encoding.
    \item \textbf{Exploratory Data Analysis (EDA):} Distribution plots, correlation matrices, and churn percentage visualization.
    \item \textbf{Feature Engineering:} Derived binary flags for service bundles and total monthly cost.
    \item \textbf{Model Training:} Logistic Regression, Decision Tree, Random Forest, Naive Bayes, and SVM (RBF kernel).
    \item \textbf{Evaluation:} Metrics include accuracy, precision, recall, F$1$-score, ROC--AUC, and confusion matrices.
    \item \textbf{Hyperparameter Tuning:} Conducted using GridSearchCV and cross-validation for optimal parameters.
    \item \textbf{Evaluation of Tuned Models:} Evaluated the models on the same parameters after the Hyperparameter Tuning.
    \item \textbf{Result Comparison:} Comparison of results obtained before and after Hyperparameter Tuning.
\end{enumerate}

% -------------------------------------------------------------------
\section{Experiments and Results}
\subsection{Initial Model Comparison}
\begin{table}[H]
\centering
\caption{TABLE I: Model Performance (Before Tuning)}
\label{tab:results_before}
\begin{tabular}{llllll}
\toprule
Model & Acc. & Prec. & Recall & F1 & AUC \\
\midrule
Logistic Regression & 0.8031 & 0.6456 & 0.5749 & 0.6082 & 0.8363 \\
Random Forest & 0.7875 & 0.6222 & 0.5107 & 0.5609 & 0.8171 \\
SVM (RBF) & 0.7868 & 0.6259 & 0.4920 & 0.5509 & 0.7909 \\
Decision Tree & 0.7186 & 0.4701 & 0.4626 & 0.4663 & 0.6366 \\
Naive Bayes & 0.6439 & 0.4179 & 0.8636 & 0.5632 & 0.8105 \\
\bottomrule
\end{tabular}
\end{table}

\noindent
\begin{itemize}
    \item \textbf{Logistic Regression:} Best overall model before tuning. Highest accuracy and ROC-AUC; balanced precision and recall. A strong, interpretable baseline for churn prediction.
    \item \textbf{Random Forest:} A strong model with high potential but slightly underperforming before tuning. It handles non-linear relationships well and is robust to noise, but recall is lower than Logistic Regression. Hyperparameter tuning (e.g., \texttt{max\_depth}, \texttt{n\_estimators}) can significantly boost its performance.
    \item \textbf{Support Vector Machine (SVM):} A solid model that performs well with scaled, high-dimensional data. It has good precision but relatively low recall. With kernel and regularization tuning, SVM can substantially improve its ROC-AUC and recall.
    \item \textbf{Decision Tree:} Prone to overfitting and weak generalization pre-tuning. It captures non-linear patterns but exhibits the lowest ROC-AUC, indicating poor discrimination. Tuning (e.g., \texttt{max\_depth}, \texttt{min\_samples\_split}) is essential for improving generalization.
    \item \textbf{Naive Bayes:} Shows extremely high recall but very low precision — meaning it identifies most churners but mislabels many non-churners. This makes it useful for early churn alerts but less reliable overall.
\end{itemize}

\noindent
\textbf{Summary:}
Linear models outperform non-linear ones before tuning. Recall remains the weakest metric; ROC-AUC is more reliable than accuracy. Naive Bayes identifies churners well but triggers many false positives.

% ------------------- ROC and Confusion Matrices (Before Tuning) -------------------
\subsection{Confusion Matrices (Before Tuning)}
\begin{figure}[H]
\centering
\includegraphics[width=0.37\textwidth]{confusion_matrix_Logistic_Regression.png}
\includegraphics[width=0.37\textwidth]{confusion_matrix_Decision_Tree.png}
\caption{Confusion Matrices for Logistic Regression and Decision Tree (before tuning).}
\label{fig:cm_before_part1}
\end{figure}

\vspace{2pt}

\begin{figure}[H]
\centering
\includegraphics[width=0.37\textwidth]{confusion_matrix_Random_Forest.png}
\includegraphics[width=0.37\textwidth]{confusion_matrix_SVM.png}
\includegraphics[width=0.37\textwidth]{confusion_matrix_Naive_Bayes.png}
\caption{Confusion Matrices for Random Forest, SVM, and Naive Bayes (before tuning).}
\label{fig:cm_before_part2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{roc_curves.png}
    \caption{ROC Curves for all models (before tuning).}
    \label{fig:roc}
    \end{figure}

\subsection{Model Optimization and Hyperparameter Tuning}
Model optimization was performed using hyperparameter tuning with GridSearchCV, where different parameter combinations were tested for each algorithm to improve accuracy, reduce error rates, and enhance overall model performance.

\subsection{Model Comparison After Tuning}
\noindent
\textbf{Logistic Regression:} It still remains the best overall model after tuning, achieving the highest accuracy among all models and the best ROC--AUC. Precision and recall remain well-balanced, and after tuning we observe slightly improved stability in the F1-score. Logistic Regression continues to be the most reliable baseline with the best overall balance of business-impact metrics.

\medskip
\noindent
\textbf{Support Vector Machine (SVM):} It showed improvement in precision and F1-score but a slight drop in AUC. After tuning, we see better precision (fewer false positives), better F1-score (improved balance between precision and recall), and a slight accuracy increase. The SVM tuning of \texttt{C}, \texttt{gamma}, and kernel parameters enhanced precision and stability. However, SVM still cannot outperform Logistic Regression because the dataset does not exhibit highly complex non-linear boundaries that strongly benefit from kernel transformations.

\medskip
\noindent
\textbf{Random Forest:} After tuning, Random Forest achieved higher precision but experienced a small decline in recall. It now has the highest precision of all models, but recall dropped, indicating that the model still misses several churners. The F1-score remains moderate, showing that while the ensemble is stable, further imbalance correction or class weighting might help.

\medskip
\noindent
\textbf{Decision Tree:} After tuning, Decision Tree demonstrates significant stability improvement. Both precision and recall improved, but it remains comparatively weak overall. The results clearly show that pruning via \texttt{max\_depth} and \texttt{min\_samples\_split} substantially improved its generalization and reduced overfitting.

\medskip
\noindent
\textbf{Naive Bayes:} Even after tuning, Naive Bayes exhibits minimal performance improvement. It maintains extremely high recall but still very low precision, leading to low overall accuracy. No major AUC gains are observed, confirming that the probabilistic assumptions limit its performance on this dataset.

\medskip
\noindent
\textbf{Summary Observations:}
\begin{itemize}[leftmargin=*]
    \item Hyperparameter tuning increased model stability rather than drastically boosting scores.
    \item Tree-based models (Random Forest, Decision Tree) would benefit from stronger imbalance handling and threshold calibration.
    \item Linear decision boundaries perform best, as confirmed by Logistic Regression remaining the top model even after tuning.
    \item High recall remains challenging across all models, indicating the inherent difficulty of identifying churners.
\end{itemize}

\begin{table}[H]
\centering
\caption{TABLE II: Model Performance (After Tuning)}
\label{tab:results_after}
\begin{tabular}{llllll}
\toprule
Model & Acc. & Prec. & Recall & F1 & AUC\\
\midrule
Logistic Regression & 0.8031 & 0.6465 & 0.5722 & 0.6071 & 0.8363\\
SVM (RBF) & 0.7989 & 0.6409 & 0.5535 & 0.5940 & 0.8273\\
Random Forest & 0.7953 & 0.6792 & 0.4358 & 0.5309 & 0.8369\\
Decision Tree & 0.7754 & 0.5980 & 0.4733 & 0.5284 & 0.8164\\
Naive Bayes & 0.6496 & 0.4224 & 0.8663 & 0.5679 & 0.8126\\
\bottomrule
\end{tabular}
\end{table}

% ------------------- Confusion Matrices (After Tuning) -------------------
\subsection{Confusion Matrices (After Tuning)}
\begin{figure}[H]
\centering
\includegraphics[width=0.47\textwidth]{confusion_matrix_after_tunningLogistic_Regression.png}
\includegraphics[width=0.47\textwidth]{confusion_matrix_after_tunningDecision_Tree.png}
\includegraphics[width=0.47\textwidth]{confusion_matrix_after_tunningRandom_Forest.png}
\caption{Confusion Matrices for Logistic Regression and Decision Tree (after tuning).}
\label{fig:cm_after_part1}
\end{figure}

\vspace{2pt}

\begin{figure}[H]
\centering
\includegraphics[width=0.47\textwidth]{confusion_matrix_after_tunningSVM.png}
\includegraphics[width=0.47\textwidth]{confusion_matrix_after_tunningNaive_Bayes.png}
\caption{Confusion Matrices for Random Forest, SVM, and Naive Bayes (after tuning).}
\label{fig:cm_after_part2}
\end{figure}

% -------------------------------------------------------------------
\subsection{ROC Curves and AUC Analysis}
\noindent
\textbf{Before Tuning:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Logistic Regression:} Achieved the highest AUC ($\approx 0.836$), showing strong ability to distinguish between churn and non-churn customers.
    \item \textbf{Random Forest and Naive Bayes:} Performed well with AUC values around $0.81$, indicating good discrimination capability.
    \item \textbf{Support Vector Machine (SVM):} Followed closely with an AUC of approximately $0.79$, reflecting solid but slightly weaker separation.
    \item \textbf{Decision Tree:} Displayed the weakest discrimination power with AUC around $0.63$, indicating a higher rate of misclassification.
\end{itemize}

\medskip
\noindent
\textbf{After Tuning:}
\begin{itemize}[leftmargin=*]
    \item AUC values improved slightly for most models, indicating better class separation and calibration.
    \item \textbf{Logistic Regression:} Remained the best overall performer with an AUC of approximately $0.836$, maintaining its top-tier stability.
    \item \textbf{Random Forest and SVM:} Both improved, approaching or exceeding AUC values of $0.83$, reflecting refined separation between churn and retention cases.
    \item \textbf{Decision Tree:} Showed significant improvement (AUC $\approx 0.81$ compared to $\approx 0.63$ earlier), becoming far more reliable after tuning.
    \item \textbf{Naive Bayes:} Improved slightly but continued to remain in the mid-performance range, with limited discrimination improvements.
\end{itemize}

\medskip
\noindent
\textbf{Overall Observations:}
\begin{itemize}[leftmargin=*]
    \item Most models exhibited stronger ROC curves and higher AUC after tuning, reflecting enhanced predictive strength.
    \item \textbf{Decision Tree} benefited the most from tuning, showing a major AUC increase.
    \item \textbf{Logistic Regression and Random Forest} consistently remained the top-performing models both before and after tuning, confirming their robustness and generalization strength.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{roc_curves_after_tunning.png}
    \caption{ROC Curves for all models (after hyperparameter tuning).}
    \label{fig:roc_after}
\end{figure}

% -------------------------------------------------------------------
\subsection{Performance Summary}

\noindent
\textbf{Best Performing Model (Both Before \& After):}
\begin{itemize}[leftmargin=*]
    \item \textbf{Logistic Regression:} Consistent, stable, and the highest accuracy overall across both pre- and post-tuning phases. It demonstrated strong reliability and minimal performance variance, making it an ideal baseline for churn prediction.
\end{itemize}

\medskip
\noindent
\textbf{Models with Most Improvement After Tuning:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Decision Tree:} Exhibited the largest performance jump, particularly in recall and overall stability after pruning and depth optimization.
    \item \textbf{SVM and Random Forest:} Achieved moderate gains in accuracy and reduced classification error through hyperparameter optimization, enhancing precision and generalization.
\end{itemize}

\medskip
\noindent
\textbf{Least Impact of Tuning:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Naive Bayes:} Showed minimal improvement due to its strong underlying independence assumptions and lack of tunable parameters.
    \item \textbf{Logistic Regression:} Remained almost unchanged, as it was already near-optimal before tuning, with stable coefficients and robust regularization.
\end{itemize}

\medskip
\noindent
\textbf{Overall Observations:}  
Hyperparameter tuning improved the performance of nearly all models except Logistic Regression, which was already well-optimized. The tuning process proved especially beneficial for complex, high-variance models such as Decision Trees, SVM, and Random Forest, resulting in better predictive accuracy, reduced overfitting, and improved model reliability.

\begin{table}[H]
    \centering
    \caption{Performance Comparison (Before and After Tuning) -- Key Metrics}
    \label{tab:perfcompare}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multicolumn{2}{c}{\textbf{Error Rate}} \\
     & \textbf{(Before)} & \textbf{(After)} & \textbf{(Before)} & \textbf{(After)} \\
    \midrule
    Logistic Regression & 0.8031 & 0.8031 & 0.1969 & 0.1969 \\
    Random Forest       & 0.7875 & 0.7953 & 0.2125 & 0.2047 \\
    SVM (RBF)           & 0.7868 & 0.7989 & 0.2132 & 0.2011 \\
    Decision Tree       & 0.7186 & 0.7754 & 0.2815 & 0.2246 \\
    Naive Bayes         & 0.6439 & 0.6496 & 0.3561 & 0.3504 \\
    \bottomrule
    \end{tabular}
    \end{table}
    
    
    

% -------------------------------------------------------------------
\subsection{Business Interpretation}
\begin{itemize}[leftmargin=*]
    \item Customers on month-to-month contracts are more likely to churn.
    \item High monthly charges correspond to elevated churn risk.
    \item Longer-term clients display a strong retention probability.
    \item Models can automate early-warning churn alerts for targeted retention campaigns.
\end{itemize}

% -------------------------------------------------------------------
\section{Conclusion}
Overall, hyperparameter tuning improved the performance of every model except Logistic Regression, which remained almost unchanged. The tuning process was especially helpful for complex, high-variance models like Decision Trees, SVM, and Random Forest, leading to better predictive accuracy and lower error rates.

% -------------------------------------------------------------------
\section*{Acknowledgments}
We thank the DA~227o teaching team for their guidance and feedback.

\begin{thebibliography}{00}
\bibitem{tyagi2025}
M.~Tyagi \textit{et al.}, ``Customer Churn Prediction – Code and Reports,'' GitHub Repository, 2025. \url{https://github.com/monikatyagiisc/customer-churn-prediction}
\bibitem{kaggle}
Kaggle, ``Telco Customer Churn,'' \url{https://www.kaggle.com/datasets/blastchar/telco-customer-churn}.
\bibitem{ibm}
Kaggle, ``Telco Customer Churn – IBM Dataset,'' \url{https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset}.
\bibitem{springer2024}
Springer, \emph{Recent Advances in Customer Churn Modeling}, 2024.
\bibitem{journals}
S.~Verma and H.~Kumar, ``A Comparative Study of Machine Learning Techniques for Customer Retention,'' \emph{International Journal of Data Science}, vol. 12, pp. 102--115, 2024.
\end{thebibliography}

\end{document}
